{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment variable\n",
    "\n",
    "open_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "\n",
    "openai_client = OpenAI(api_key = open_api_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 01:14:39,223 - INFO - Opening file selection dialog.\n",
      "2025-03-01 01:14:42,025 - INFO - Selected PDF file: C:/Users/derri/Downloads/Nuneaton Hose and Fittings Full Catalogue.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF split successfully!\n",
      "First part saved as: part1.pdf\n",
      "Second part saved as: part2.pdf\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "\n",
    "# from modules.pdf_extraction import select_pdf_file\n",
    "# from modules.pdf_extraction import extract_text_from_pages\n",
    "\n",
    "# pdf_path = select_pdf_file()\n",
    "\n",
    "# def split_pdf(input_path, output_path_1, output_path_2, split_page):\n",
    "#     \"\"\"\n",
    "#     Split a PDF file into two separate PDF files.\n",
    "    \n",
    "#     Args:\n",
    "#         input_path (str): Path to the input PDF file\n",
    "#         output_path_1 (str): Path where to save the first part\n",
    "#         output_path_2 (str): Path where to save the second part\n",
    "#         split_page (int): The page number where to split (this page will be the first page of the second PDF)\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Create PDF reader object\n",
    "#         reader = PdfReader(input_path)\n",
    "        \n",
    "#         # Get total number of pages\n",
    "#         total_pages = len(reader.pages)\n",
    "        \n",
    "#         if split_page >= total_pages:\n",
    "#             raise ValueError(\"Split page number cannot be greater than total pages\")\n",
    "        \n",
    "#         # Create two PDF writer objects\n",
    "#         writer1 = PdfWriter()\n",
    "#         writer2 = PdfWriter()\n",
    "        \n",
    "#         # Add pages to first output PDF (before split point)\n",
    "#         for page in range(split_page):\n",
    "#             writer1.add_page(reader.pages[page])\n",
    "            \n",
    "#         # Add pages to second output PDF (from split point to end)\n",
    "#         for page in range(split_page, total_pages):\n",
    "#             writer2.add_page(reader.pages[page])\n",
    "            \n",
    "#         # Save the first part\n",
    "#         with open(output_path_1, 'wb') as output1:\n",
    "#             writer1.write(output1)\n",
    "            \n",
    "#         # Save the second part\n",
    "#         with open(output_path_2, 'wb') as output2:\n",
    "#             writer2.write(output2)\n",
    "            \n",
    "#         return True\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {str(e)}\")\n",
    "#         return False\n",
    "\n",
    "# def main():\n",
    "#     # Example usage\n",
    "#     input_file = pdf_path  # Replace with your PDF file\n",
    "#     output_file1 = \"part1.pdf\"\n",
    "#     output_file2 = \"part2.pdf\"\n",
    "#     split_at_page = 1000   # Split after first page (0-based index)\n",
    "    \n",
    "#     if os.path.exists(input_file):\n",
    "#         success = split_pdf(input_file, output_file1, output_file2, split_at_page)\n",
    "#         if success:\n",
    "#             print(f\"PDF split successfully!\")\n",
    "#             print(f\"First part saved as: {output_file1}\")\n",
    "#             print(f\"Second part saved as: {output_file2}\")\n",
    "#         else:\n",
    "#             print(\"Failed to split PDF.\")\n",
    "#     else:\n",
    "#         print(f\"Input file '{input_file}' not found.\")\n",
    "        \n",
    "# main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 01:12:37,991 - INFO - Opening file selection dialog.\n",
      "2025-03-01 01:12:43,216 - INFO - Selected PDF file: C:/Users/derri/Downloads/Nuneaton Hose and Fittings Full Catalogue.pdf\n",
      "2025-03-01 01:12:43,218 - INFO - Starting text extraction from PDF.\n",
      "2025-03-01 01:12:43,534 - INFO - Completed text extraction.\n"
     ]
    }
   ],
   "source": [
    "# 15\n",
    "\n",
    "from modules.pdf_extraction import select_pdf_file\n",
    "from modules.pdf_extraction import extract_text_from_pages\n",
    "\n",
    "pdf_path = select_pdf_file()\n",
    "page_no = 0\n",
    "extracted_text = extract_text_from_pages(pdf_path, pages=page_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf \n",
    "## 111\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "import importlib\n",
    "import modules.llm\n",
    "import modules.pdf_extraction\n",
    "\n",
    "\n",
    "from modules.pdf_extraction import get_page_pixel_data\n",
    "\n",
    "\n",
    "\n",
    "# this may have to happen on a page by page basis\n",
    "\n",
    "base64_image = get_page_pixel_data(pdf_path=pdf_path, page_no=page_no, \n",
    "                    dpi = 500, image_type = 'png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymupdf\n",
    "\n",
    "# # Open some document, for example a PDF (could also be EPUB, XPS, etc.)\n",
    "# doc = pymupdf.open(pdf_path)\n",
    "\n",
    "# # Load a desired page. This works via 0-based numbers\n",
    "# page = doc[0]  # this is the first page\n",
    "\n",
    "# # Look for tables on this page and display the table count\n",
    "# tabs = page.find_tables()\n",
    "# num_tables = len(tabs.tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.llm\n",
    "importlib.reload(modules.llm)\n",
    "from modules.llm import table_identification_llm\n",
    "\n",
    "output_table_identification_llm = await table_identification_llm(text_input=extracted_text,  base64_image=base64_image, open_api_key=open_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_table_identification_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(modules.pdf_extraction)\n",
    "from modules.pdf_extraction import get_validated_table_info\n",
    "num_tables, table_headers, table_location, confidence_score_0 = await get_validated_table_info(text_input=extracted_text, open_api_key=open_api_key, base64_image= base64_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modules.pdf_extraction import table_identification_llm\n",
    "\n",
    "\n",
    "# output = await table_identification_llm(text_input=extracted_text, base64_image=base64_image, open_api_key=open_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_tables == 0:\n",
    "    raise ValueError(\"No tables found on the page\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in control for no tables found\n",
    "# table_with_index = \" || \".join([f\"index {i}: {header}\" for i, header in enumerate(table_headers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables_data = [f\"[{i}]\" for i in table_headers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import importlib\n",
    "import modules.pdf_extraction\n",
    "importlib.reload(modules.pdf_extraction)\n",
    "from modules.pdf_extraction import process_tables_to_df\n",
    "\n",
    "\n",
    "user_input = \"I would like to get all the data text from the table\"\n",
    "\n",
    "output_1 = await process_tables_to_df(\n",
    "                     user_text=user_input,\n",
    "                     table_location = table_location,\n",
    "                     table_headers = table_headers,\n",
    "                     extracted_text=extracted_text, \n",
    "                     base64_image=base64_image, \n",
    "                     open_api_key=open_api_key)\n",
    "\n",
    "\n",
    "output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add in try and except for error handling\n",
    "\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import importlib\n",
    "# import modules.llm\n",
    "# importlib.reload(modules.llm)\n",
    "# from modules.llm import vision_llm_parser\n",
    "\n",
    "# user_input = \"I would like to get all the data text from the table\"\n",
    "\n",
    "# output = await vision_llm_parser(\n",
    "#                 user_text=user_input,\n",
    "#                 text_input=extracted_text,\n",
    "#                 table_to_target=table_headers[1],\n",
    "#                 base64_image=base64_image,\n",
    "#                 open_api_key=open_api_key,\n",
    "#                 model='gpt-4o'\n",
    "#             )\n",
    "\n",
    "# # from modules.pdf_extraction import parse_variable_data_to_df\n",
    "# # df_0 = parse_variable_data_to_df(output)\n",
    "# # df_0\n",
    "\n",
    "# # def extract_list_from_string(text):\n",
    "# #     # Find the content between the first [ and last ], including the brackets\n",
    "# #     match = re.search(r'(\\[.*\\])', text, re.DOTALL)\n",
    "# #     if match:\n",
    "# #         # Return the full string including brackets\n",
    "# #         return match.group(1)\n",
    "# #     return None\n",
    "\n",
    "# # extracted_dict = extract_list_from_string(output)\n",
    "\n",
    "\n",
    "# # data = json.loads(extracted_dict)\n",
    "# # df = pd.DataFrame(data)\n",
    "# # df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_path = select_pdf_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymupdf\n",
    "\n",
    "# # Open some document, for example a PDF (could also be EPUB, XPS, etc.)\n",
    "# doc = pymupdf.open(pdf_path)\n",
    "\n",
    "# # Load a desired page. This works via 0-based numbers\n",
    "# page = doc[0]  # this is the first page\n",
    "\n",
    "# # Look for tables on this page and display the table count\n",
    "# tabs = page.find_tables()\n",
    "# print(f\"{len(tabs.tables)} table(s) on {page}\")\n",
    "\n",
    "# # We will see a message like \"1 table(s) on page 0 of input.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 22:51:09,456 - INFO - Opening file selection dialog.\n",
      "2025-02-28 22:51:18,213 - INFO - Selected PDF file: C:/Users/derri/Downloads/Nuneaton Hose and Fittings Full Catalogue.pdf\n"
     ]
    }
   ],
   "source": [
    "from modules.pdf_extraction import select_pdf_file\n",
    "\n",
    "from modules.pdf_extraction import get_validated_table_info\n",
    "from modules.pdf_extraction import get_page_pixel_data\n",
    "from modules.pdf_extraction import process_tables_to_df\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import os\n",
    "import pymupdf\n",
    "import time\n",
    "\n",
    "pdf_path = select_pdf_file()\n",
    "doc = pymupdf.open(pdf_path)\n",
    "total_pages = doc.page_count  # total number of pages in the document\n",
    "page_indices = range(total_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "page 20 of C:/Users/derri/Downloads/Nuneaton Hose and Fittings Full Catalogue.pdf"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15\\n1.01  Hydraulic Hose & Inserts\\nHydraulics\\n1\\n 1/2” EN853-2 Breaker Hoses\\nComprising of 2 x 6 metre clamped together lengths of high quality 1/2”\\nEN853-2 hydraulic hose, complete with 1/2” BSPP male end m ttings.\\n1/2” BSPP Male x Male, \\n6 Metre\\n0400\\nBH08-08M08M-06MT\\nTech Sheet 20002\\nComprising of 2 x 9 metre clamped together lengths of high quality 1/2” \\nEN853-2 hydraulic hose, complete with 1/2” BSPP male end m ttings.\\n1/2” BSPP Male x Male, \\n9 Metre\\n0400\\nBH08-08M08M-09MT\\nTech Sheet 20002\\nComprising of 2 x 6 metre clamped together lengths of high quality 1/2” \\nEN853-2 hydraulic hose, complete with 2 sets of n at face QRCs.\\nFlat Face Quick Release Couplings, \\n6 Metre\\n0400\\nBH08-08Q08P-06MT\\nTech Sheet 20002\\nComprising of 2 x 9 metre clamped together lengths of high quality 1/2”\\nEN853-2 hydraulic hose, complete with 2 sets of n at face QRCs.\\nFlat Face Quick Release Couplings, \\n9 Metre\\n0400\\nBH08-08Q08P-09MT\\nTech Sheet 20002\\nLeader hoses comprising of 2 x 0.5 metre clamped together lengths of high quality 1/2” \\nEN853-2 hydraulic hose, complete with 1 set of n at face QRCs one end and 1/2” BSPP \\nmales the other.\\nLeader Hose, Flat Face QRC x 1/2” \\nBSPP Male, 0.5 Metre\\n0400\\nBH08-08Q08M-.5MT\\nTech Sheet 20002\\nOther lengths available on request\\nBREAKER HOSE ASSEMBLIES\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = doc.load_page(20)\n",
    "page.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "\n",
    "for i in  range(1562,1567):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 23:08:08,057 - INFO - Opening file selection dialog.\n",
      "2025-02-28 23:08:25,525 - INFO - Selected PDF file: C:/Users/derri/Downloads/Nuneaton Hose and Fittings Full Catalogue.pdf\n",
      "2025-02-28 23:08:25,680 - INFO - Converting PDF page 1563 to base64 image. DPI=500, Format=png\n",
      "2025-02-28 23:08:26,301 - INFO - Finished converting page to base64.\n",
      "2025-02-28 23:08:26,304 - INFO - Validating table information with multiple LLM calls.\n",
      "2025-02-28 23:08:33,880 - INFO - Initial table info match or same table count. Returning first attempt's result.\n",
      "2025-02-28 23:08:33,887 - INFO - Converting PDF page 1564 to base64 image. DPI=500, Format=png\n",
      "2025-02-28 23:08:34,461 - INFO - Finished converting page to base64.\n",
      "2025-02-28 23:08:34,464 - INFO - Validating table information with multiple LLM calls.\n",
      "2025-02-28 23:08:34,465 - INFO - Processing tables to DataFrame for page 1563\n",
      "2025-02-28 23:08:49,861 - INFO - Majority match found with second and third results.\n",
      "2025-02-28 23:08:49,863 - INFO - Converting PDF page 1565 to base64 image. DPI=500, Format=png\n",
      "2025-02-28 23:08:50,383 - INFO - Finished converting page to base64.\n",
      "2025-02-28 23:08:50,386 - INFO - Validating table information with multiple LLM calls.\n",
      "2025-02-28 23:08:50,386 - INFO - Processing tables to DataFrame for page 1564\n",
      "2025-02-28 23:08:55,885 - INFO - Initial table info match or same table count. Returning first attempt's result.\n",
      "2025-02-28 23:08:55,892 - INFO - Converting PDF page 1566 to base64 image. DPI=500, Format=png\n",
      "2025-02-28 23:08:56,478 - INFO - Finished converting page to base64.\n",
      "2025-02-28 23:08:56,480 - INFO - Validating table information with multiple LLM calls.\n",
      "2025-02-28 23:08:56,480 - INFO - Processing tables to DataFrame for page 1565\n",
      "2025-02-28 23:09:03,660 - INFO - Initial table info match or same table count. Returning first attempt's result.\n",
      "2025-02-28 23:09:03,667 - INFO - Converting PDF page 1567 to base64 image. DPI=500, Format=png\n",
      "2025-02-28 23:09:04,234 - INFO - Finished converting page to base64.\n",
      "2025-02-28 23:09:04,237 - INFO - Validating table information with multiple LLM calls.\n",
      "2025-02-28 23:09:04,238 - INFO - Processing tables to DataFrame for page 1566\n",
      "2025-02-28 23:09:04,271 - INFO - Successfully retrieved data using model 'gpt-4o'.\n",
      "2025-02-28 23:09:04,274 - INFO - Completed processing tables to DataFrame for page 1565.\n",
      "2025-02-28 23:09:04,701 - INFO - Successfully retrieved data using model 'gpt-4o'.\n",
      "2025-02-28 23:09:04,708 - INFO - Completed processing tables to DataFrame for page 1563.\n",
      "2025-02-28 23:09:05,731 - INFO - Successfully retrieved data using model 'gpt-4o'.\n",
      "2025-02-28 23:09:05,736 - INFO - Completed processing tables to DataFrame for page 1564.\n",
      "2025-02-28 23:09:13,281 - INFO - Successfully retrieved data using model 'gpt-4o'.\n",
      "2025-02-28 23:09:13,283 - WARNING - Could not extract table with index 1 on page 1566 , skipping.\n",
      "2025-02-28 23:09:13,284 - ERROR - No tables extracted from the string.\n",
      "2025-02-28 23:09:13,284 - WARNING - Could not extract table with index 2 on page 1566 , skipping.\n",
      "2025-02-28 23:09:13,285 - ERROR - No tables extracted from the string.\n",
      "2025-02-28 23:09:13,286 - WARNING - Could not extract table with index 3 on page 1566 , skipping.\n",
      "2025-02-28 23:09:13,286 - INFO - Completed processing tables to DataFrame for page 1566.\n",
      "2025-02-28 23:09:17,613 - INFO - Majority match found with second and third results.\n",
      "2025-02-28 23:09:17,613 - INFO - Processing tables to DataFrame for page 1567\n",
      "2025-02-28 23:09:23,922 - INFO - Successfully retrieved data using model 'gpt-4o'.\n",
      "2025-02-28 23:09:23,925 - WARNING - Could not extract table with index 1 on page 1567 , skipping.\n",
      "2025-02-28 23:09:23,926 - INFO - Completed processing tables to DataFrame for page 1567.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# add in try when it can not extract from string\n",
    "# # remove o1 and pop in try at that point where it can not find the string, try 3 times till it does. \n",
    "\n",
    "# could pop \n",
    "\n",
    "from modules.pdf_extraction import select_pdf_file\n",
    "\n",
    "from modules.pdf_extraction import get_validated_table_info\n",
    "from modules.pdf_extraction import get_page_pixel_data\n",
    "from modules.pdf_extraction import process_tables_to_df\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import os\n",
    "import pymupdf\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# to do.. table names headers can be dupilcated may be helpful to use say table 1,2, etc to deitigusih tem. \n",
    "# use pdf plumber  page.find_tables() and gpt table count for confidence calculation. \n",
    "# the same table can be extraected twice or more if the header is not clear\n",
    "# extract header witout the text \n",
    "\n",
    "file_name = 'test_7'    \n",
    "user_text='Extract all data from the table(s). The first column in the table is usually the part number. The column names for these are usually numeric (eg: 0309, 0210, etc) rename these inital columns to \"part number'\n",
    "\n",
    "# 1. Load Credientials\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "# Get the API key from the environment variable\n",
    "open_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI(api_key = open_api_key)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Select PDF file and extract text\n",
    "pdf_path = select_pdf_file()\n",
    "doc = pymupdf.open(pdf_path)\n",
    "total_pages = doc.page_count  # total number of pages in the document\n",
    "page_indices = range(1562,1567) #range(total_pages)\n",
    "\n",
    "# page_indices can be a list of page numbers to process\n",
    "\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "async def process_page():\n",
    "    tasks = []\n",
    "    results_output = []\n",
    "    # Create all tasks first \n",
    "    async with asyncio.TaskGroup() as tg:\n",
    "        for page_no in page_indices:\n",
    "            page = doc.load_page(page_no)\n",
    "            extracted_text = page.get_text()\n",
    "            \n",
    "            # extracted_text = extract_text_from_pages(pdf_path, pages=page_no)\n",
    "            base64_image = get_page_pixel_data(pdf_path=pdf_path, page_no=page_no, \n",
    "                                dpi = 500, image_type = 'png')\n",
    "        \n",
    "            num_tables, table_headers, table_location, confidence_score_0 = await get_validated_table_info(\n",
    "                text_input=extracted_text, \n",
    "                open_api_key=open_api_key, \n",
    "                base64_image=base64_image\n",
    "            )\n",
    "\n",
    "            if num_tables == 0:\n",
    "                print(f\"No tables found on page {page_no + 1}, skipping...\")\n",
    "                continue\n",
    "    \n",
    "            tasks.append(tg.create_task(process_tables_to_df(\n",
    "                table_headers, \n",
    "                table_location,\n",
    "                user_text, \n",
    "                extracted_text, \n",
    "                base64_image, \n",
    "                open_api_key,\n",
    "                page_number=page_no)))\n",
    "            \n",
    "        # Await all tasks to complete\n",
    "        for task in tasks:\n",
    "            results_output.append(await task)\n",
    "    \n",
    "    if not results_output:\n",
    "        raise ValueError(\"No tables found on any of the processed pages\")\n",
    "            \n",
    "    # df_out_1 = pd.concat(results_output, ignore_index=True)\n",
    "    return results_output\n",
    "\n",
    "\n",
    "output_final = await process_page()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 23:11:35,989 - INFO - Writing output to Excel at 'files/NN_Test_1_combined.xlsx' with option=1.\n",
      "2025-02-28 23:11:36,044 - INFO - Excel file writing complete.\n"
     ]
    }
   ],
   "source": [
    "from modules.pdf_extraction import write_output_final\n",
    "write_output_final(output_final, excel_path='files/NN_Test_1_combined.xlsx', option=1, gap_rows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import itertools\n",
    "# all_dfs = list(itertools.chain.from_iterable(output_final))\n",
    "# combined_dfs = pd.concat(all_dfs)\n",
    "\n",
    "\n",
    "# combined_dfs['descriptor_label'] = (\n",
    "#     combined_dfs.groupby('page_number')['table_header_descriptor']\n",
    "#     .transform(lambda x: pd.factorize(x)[0] + 1)\n",
    "# )\n",
    "\n",
    "\n",
    "# combined_dfs['page_table'] = (\n",
    "#     'page ' + combined_dfs['page_number'].astype(str) +\n",
    "#     ' table ' + combined_dfs['descriptor_label'].astype(str)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_dict = combined_dfs.groupby('table_header_descriptor')['page_table'].apply(list).to_dict()\n",
    "# mapping_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['table_header_descriptor'].map(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "\n",
    "# all_dfs = list(itertools.chain.from_iterable(output_final))\n",
    "# combined_dfs = pd.concat(all_dfs)\n",
    "# combined_dfs['table_header_descriptor_factorized'], _ = pd.factorize(combined_dfs['table_header_descriptor'])\n",
    "\n",
    "\n",
    "\n",
    "# unique_headers = combined_dfs['table_header_descriptor_factorized'].unique()\n",
    "# header_to_num = {\n",
    "#     header: f\"Page {combined_dfs['page_number'] + 1} Table {i+1}\"\n",
    "#     for i, header in enumerate(unique_headers)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate a \"page_table\" label using unique descriptors\n",
    "# unique_headers = df['table_header_descriptor'].unique()\n",
    "# header_to_num = {\n",
    "#     header: f\"Page {df['page_number'] + 1} Table {i+1}\"\n",
    "#     for i, header in enumerate(unique_headers)\n",
    "# }\n",
    "# df['page_table'] = df['table_header_descriptor'].map(header_to_num)\n",
    "\n",
    "# # df.drop(columns=['table_header_descriptor'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write combined tables to excel\n",
    "\n",
    "df_combined = pd.concat(output_final[0], ignore_index=True)\n",
    "df_combined.to_excel(f'files/{file_name}.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.pdf_extraction import write_split_tables_to_excel\n",
    "\n",
    "# Call the function with output_final[0]\n",
    "write_split_tables_to_excel(output_final[0],output_path='files/test_output.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
