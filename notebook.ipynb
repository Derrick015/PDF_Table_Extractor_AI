{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment variable\n",
    "\n",
    "open_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "\n",
    "openai_client = OpenAI(api_key = open_api_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15\n",
    "\n",
    "from modules.pdf_extraction import select_pdf_file\n",
    "from modules.pdf_extraction import extract_text_from_pages\n",
    "\n",
    "pdf_path = select_pdf_file()\n",
    "page_no = 0\n",
    "extracted_text = extract_text_from_pages(pdf_path, pages=page_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf \n",
    "## 111\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "import importlib\n",
    "import modules.llm\n",
    "import modules.pdf_extraction\n",
    "\n",
    "\n",
    "from modules.pdf_extraction import get_page_pixel_data\n",
    "\n",
    "\n",
    "\n",
    "# this may have to happen on a page by page basis\n",
    "\n",
    "base64_image = get_page_pixel_data(pdf_path=pdf_path, page_no=page_no, \n",
    "                    dpi = 500, image_type = 'png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymupdf\n",
    "\n",
    "# # Open some document, for example a PDF (could also be EPUB, XPS, etc.)\n",
    "# doc = pymupdf.open(pdf_path)\n",
    "\n",
    "# # Load a desired page. This works via 0-based numbers\n",
    "# page = doc[0]  # this is the first page\n",
    "\n",
    "# # Look for tables on this page and display the table count\n",
    "# tabs = page.find_tables()\n",
    "# num_tables = len(tabs.tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.llm\n",
    "importlib.reload(modules.llm)\n",
    "from modules.llm import table_identification_llm\n",
    "\n",
    "output_table_identification_llm = await table_identification_llm(text_input=extracted_text,  base64_image=base64_image, open_api_key=open_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_table_identification_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(modules.pdf_extraction)\n",
    "from modules.pdf_extraction import get_validated_table_info\n",
    "num_tables, table_headers, table_location, confidence_score_0 = await get_validated_table_info(text_input=extracted_text, open_api_key=open_api_key, base64_image= base64_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modules.pdf_extraction import table_identification_llm\n",
    "\n",
    "\n",
    "# output = await table_identification_llm(text_input=extracted_text, base64_image=base64_image, open_api_key=open_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_tables == 0:\n",
    "    raise ValueError(\"No tables found on the page\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in control for no tables found\n",
    "# table_with_index = \" || \".join([f\"index {i}: {header}\" for i, header in enumerate(table_headers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables_data = [f\"[{i}]\" for i in table_headers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import importlib\n",
    "import modules.pdf_extraction\n",
    "importlib.reload(modules.pdf_extraction)\n",
    "from modules.pdf_extraction import process_tables_to_df\n",
    "\n",
    "\n",
    "user_input = \"I would like to get all the data text from the table\"\n",
    "\n",
    "output_1 = await process_tables_to_df(\n",
    "                     user_text=user_input,\n",
    "                     table_location = table_location,\n",
    "                     table_headers = table_headers,\n",
    "                     extracted_text=extracted_text, \n",
    "                     base64_image=base64_image, \n",
    "                     open_api_key=open_api_key)\n",
    "\n",
    "\n",
    "output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add in try and except for error handling\n",
    "\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import importlib\n",
    "# import modules.llm\n",
    "# importlib.reload(modules.llm)\n",
    "# from modules.llm import vision_llm_parser\n",
    "\n",
    "# user_input = \"I would like to get all the data text from the table\"\n",
    "\n",
    "# output = await vision_llm_parser(\n",
    "#                 user_text=user_input,\n",
    "#                 text_input=extracted_text,\n",
    "#                 table_to_target=table_headers[1],\n",
    "#                 base64_image=base64_image,\n",
    "#                 open_api_key=open_api_key,\n",
    "#                 model='gpt-4o'\n",
    "#             )\n",
    "\n",
    "# # from modules.pdf_extraction import parse_variable_data_to_df\n",
    "# # df_0 = parse_variable_data_to_df(output)\n",
    "# # df_0\n",
    "\n",
    "# # def extract_list_from_string(text):\n",
    "# #     # Find the content between the first [ and last ], including the brackets\n",
    "# #     match = re.search(r'(\\[.*\\])', text, re.DOTALL)\n",
    "# #     if match:\n",
    "# #         # Return the full string including brackets\n",
    "# #         return match.group(1)\n",
    "# #     return None\n",
    "\n",
    "# # extracted_dict = extract_list_from_string(output)\n",
    "\n",
    "\n",
    "# # data = json.loads(extracted_dict)\n",
    "# # df = pd.DataFrame(data)\n",
    "# # df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_path = select_pdf_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymupdf\n",
    "\n",
    "# # Open some document, for example a PDF (could also be EPUB, XPS, etc.)\n",
    "# doc = pymupdf.open(pdf_path)\n",
    "\n",
    "# # Load a desired page. This works via 0-based numbers\n",
    "# page = doc[0]  # this is the first page\n",
    "\n",
    "# # Look for tables on this page and display the table count\n",
    "# tabs = page.find_tables()\n",
    "# print(f\"{len(tabs.tables)} table(s) on {page}\")\n",
    "\n",
    "# # We will see a message like \"1 table(s) on page 0 of input.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 22:03:40,844 - INFO - Opening file selection dialog.\n",
      "2025-02-28 22:03:45,385 - INFO - Selected PDF file: C:/Users/derri/Desktop/extracted 1 and 2.pdf\n",
      "2025-02-28 22:03:45,425 - INFO - Converting PDF page 1 to base64 image. DPI=500, Format=png\n",
      "2025-02-28 22:03:47,809 - INFO - Finished converting page to base64.\n",
      "2025-02-28 22:03:47,815 - INFO - Validating table information with multiple LLM calls.\n",
      "2025-02-28 22:04:05,552 - INFO - Majority match found with first and third results.\n",
      "2025-02-28 22:04:05,561 - INFO - Converting PDF page 2 to base64 image. DPI=500, Format=png\n",
      "2025-02-28 22:04:07,586 - INFO - Finished converting page to base64.\n",
      "2025-02-28 22:04:07,591 - INFO - Validating table information with multiple LLM calls.\n",
      "2025-02-28 22:04:07,592 - INFO - Processing tables to DataFrame for page 1\n",
      "2025-02-28 22:04:18,603 - INFO - Initial table info match or same table count. Returning first attempt's result.\n",
      "2025-02-28 22:04:18,604 - INFO - Processing tables to DataFrame for page 2\n",
      "2025-02-28 22:04:24,255 - INFO - Successfully retrieved data using model 'gpt-4o'.\n",
      "2025-02-28 22:04:24,260 - INFO - Completed processing tables to DataFrame for page 1.\n",
      "2025-02-28 22:04:36,796 - INFO - Successfully retrieved data using model 'gpt-4o'.\n",
      "2025-02-28 22:04:36,798 - INFO - Completed processing tables to DataFrame for page 2.\n"
     ]
    }
   ],
   "source": [
    "from modules.pdf_extraction import select_pdf_file\n",
    "\n",
    "from modules.pdf_extraction import get_validated_table_info\n",
    "from modules.pdf_extraction import get_page_pixel_data\n",
    "from modules.pdf_extraction import process_tables_to_df\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import os\n",
    "import pymupdf\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# to do.. table names headers can be dupilcated may be helpful to use say table 1,2, etc to deitigusih tem. \n",
    "# use pdf plumber  page.find_tables() and gpt table count for confidence calculation. \n",
    "# the same table can be extraected twice or more if the header is not clear\n",
    "# extract header witout the text \n",
    "\n",
    "file_name = 'test_7'    \n",
    "user_text='Extract all data from the table(s) the header'\n",
    "\n",
    "# 1. Load Credientials\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "# Get the API key from the environment variable\n",
    "open_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI(api_key = open_api_key)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Select PDF file and extract text\n",
    "pdf_path = select_pdf_file()\n",
    "doc = pymupdf.open(pdf_path)\n",
    "total_pages = doc.page_count  # total number of pages in the document\n",
    "page_indices = range(total_pages)\n",
    "\n",
    "# page_indices can be a list of page numbers to process\n",
    "\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "async def process_page():\n",
    "    tasks = []\n",
    "    results_output = []\n",
    "    # Create all tasks first \n",
    "    async with asyncio.TaskGroup() as tg:\n",
    "        for page_no in page_indices:\n",
    "            page = doc.load_page(page_no)\n",
    "            extracted_text = page.get_text()\n",
    "            \n",
    "            # extracted_text = extract_text_from_pages(pdf_path, pages=page_no)\n",
    "            base64_image = get_page_pixel_data(pdf_path=pdf_path, page_no=page_no, \n",
    "                                dpi = 500, image_type = 'png')\n",
    "        \n",
    "            num_tables, table_headers, table_location, confidence_score_0 = await get_validated_table_info(\n",
    "                text_input=extracted_text, \n",
    "                open_api_key=open_api_key, \n",
    "                base64_image=base64_image\n",
    "            )\n",
    "\n",
    "            if num_tables == 0:\n",
    "                print(f\"No tables found on page {page_no + 1}, skipping...\")\n",
    "                continue\n",
    "    \n",
    "            tasks.append(tg.create_task(process_tables_to_df(\n",
    "                table_headers, \n",
    "                table_location,\n",
    "                user_text, \n",
    "                extracted_text, \n",
    "                base64_image, \n",
    "                open_api_key,\n",
    "                page_number=page_no)))\n",
    "            \n",
    "        # Await all tasks to complete\n",
    "        for task in tasks:\n",
    "            results_output.append(await task)\n",
    "    \n",
    "    if not results_output:\n",
    "        raise ValueError(\"No tables found on any of the processed pages\")\n",
    "            \n",
    "    # df_out_1 = pd.concat(results_output, ignore_index=True)\n",
    "    return results_output\n",
    "\n",
    "\n",
    "output_final = await process_page()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.pdf_extraction import write_output_final\n",
    "write_output_final(output_final, excel_path='files/AllTables12.xlsx', option=2, gap_rows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import itertools\n",
    "# all_dfs = list(itertools.chain.from_iterable(output_final))\n",
    "# combined_dfs = pd.concat(all_dfs)\n",
    "\n",
    "\n",
    "# combined_dfs['descriptor_label'] = (\n",
    "#     combined_dfs.groupby('page_number')['table_header_descriptor']\n",
    "#     .transform(lambda x: pd.factorize(x)[0] + 1)\n",
    "# )\n",
    "\n",
    "\n",
    "# combined_dfs['page_table'] = (\n",
    "#     'page ' + combined_dfs['page_number'].astype(str) +\n",
    "#     ' table ' + combined_dfs['descriptor_label'].astype(str)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_dict = combined_dfs.groupby('table_header_descriptor')['page_table'].apply(list).to_dict()\n",
    "# mapping_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['table_header_descriptor'].map(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "\n",
    "# all_dfs = list(itertools.chain.from_iterable(output_final))\n",
    "# combined_dfs = pd.concat(all_dfs)\n",
    "# combined_dfs['table_header_descriptor_factorized'], _ = pd.factorize(combined_dfs['table_header_descriptor'])\n",
    "\n",
    "\n",
    "\n",
    "# unique_headers = combined_dfs['table_header_descriptor_factorized'].unique()\n",
    "# header_to_num = {\n",
    "#     header: f\"Page {combined_dfs['page_number'] + 1} Table {i+1}\"\n",
    "#     for i, header in enumerate(unique_headers)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate a \"page_table\" label using unique descriptors\n",
    "# unique_headers = df['table_header_descriptor'].unique()\n",
    "# header_to_num = {\n",
    "#     header: f\"Page {df['page_number'] + 1} Table {i+1}\"\n",
    "#     for i, header in enumerate(unique_headers)\n",
    "# }\n",
    "# df['page_table'] = df['table_header_descriptor'].map(header_to_num)\n",
    "\n",
    "# # df.drop(columns=['table_header_descriptor'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write combined tables to excel\n",
    "\n",
    "df_combined = pd.concat(output_final[0], ignore_index=True)\n",
    "df_combined.to_excel(f'files/{file_name}.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.pdf_extraction import write_split_tables_to_excel\n",
    "\n",
    "# Call the function with output_final[0]\n",
    "write_split_tables_to_excel(output_final[0],output_path='files/test_output.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
